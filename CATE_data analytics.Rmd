---
title: "Project: CATE"
subtitle: "Data Preparation (chapter 4) AI (chapter 5)"
date: 2018-08-24  
output:
  word_document: default
  pdf_document: default
  html_document:
  code_folding: hide
  df_print: paged
---

# Data Understanding/Exploration Topics

Overall dataset check if matching requirements (completeness, volumes, labels, accessability)

* Summary statistics (incl. mean, median, variations, …)
* Five numbers summary by boxplots (numerical data)
* Distribution by bar charts and histograms
* Categories reduction
* Derived variables
* Missing values and outliers
* Subgroups, patterns and trends
* Covariations (relationships between pairs of attributes)
    - distribution of a continuous variable broken down by a categorical variable
    - between categorical variables
    - between two continuous variables
* Correlations (numerical variables)

# Hypotheses to be examined by data analytics activities

Repaired boards (based on optical test) produce high number of pseudo errors.

ICT measurement threshold violation distribution over components is quite 'zerklüftet' / fissured -> certain set of components are responsible for majority of violations.

Related to above hypothesis is the observation that violations are concentrated on some locations on the board. Therefore location info will be candidate input variable for model building.

Trend diagrams of measurement cycle failures how strong peaks with high number of failures.  
--> Data analytics need to examine whether this is due to variation in tested boards or dependent on external influence (manufacturing conditions) 

Distribution of varco deviation accross all components:
--> Data analytics will have to focus on those components which show high variation in the FAIL measurement cycles.

#1. Setup Environment
Provide filenames, ~type and environment variables. 
Source data exploration routines.
Source model evaluation routines.

```{r, results='hide', echo = FALSE, message=FALSE}
# Libraries 
library(readxl)
library(tidyverse)
library(hexbin)
# Parameters 
rel_fpath  <- "./data/"    # relative directory where original data is stored
filename ='A5E01283425_30062016_25072016' # file name of the original data 
filename2 ='A5E02758387_01062016_25072016' # file name of the original data
filename3 ='A5E02758387_11072016_15082016' # file name of the original data
filename4 ='A5E31281377_15062016_11072016_BGmitwenigenFehlern' # file name of the original data
filename5 ='A5E32692783_25072016_15082016' # file name of the original data
filename6 ='BD_CIB_DAC-A5E33212840_01062016_01072016' # file name of the original data
filename7 ='CIB_DAC-A5E33212840_01072016_25072016' # file name of the original data
filename8 ='RAWDATA_A5E02758387_semicolon' # file name of the original data

endung ='.csv'            # file extension of the original data
rel_fpath_output <- "./data/" # relative directory where outputs are stored
outputcsv  <- "./output/clean_4_modelling_with_R.csv"

Read_Fresh_from_File <- F  # Set to TRUE for first run to fill var rawframe
                           # clean dat frame, set to FALSE for initial data understanding
options(scipen=999)        # suppress exponential form of print output format
shortnames <- TRUE         # if T change to short variable names is desired
printpdf <- FALSE          # if printout in PDF desired
scatterplot <- FALSE       # it T do scatterplot
basic_tranform <- F        # do basic feature transformations
# ====================== Sources ===============
source(file = "Functions_data_exploration.R")
source(file = "Functions_modelevaluation.R")
```
#2. Data Understanding
  
##2.1 Collect Initial Data

Routines:  
Read from data sources from "rel_fpath"!  
Read description and configuration file from "rel_fpath"!  
Save data as RDS file format 

###2.1.1 Collect ICT measurement Data
Read 8 ICT measurement files.

A5E01283425_30062016_25072016.csv
A5E02758387_01062016_25072016.csv
A5E02758387_11072016_15082016.csv
A5E31281377_15062016_11072016_BGmitwenigenFehlern.csv
A5E32692783_25072016_15082016.csv
BD_CIB_DAC-A5E33212840_01062016_01072016.csv
CIB_DAC-A5E33212840_01072016_25072016.csv
RAWDATA_A5E02758387_semicolon.csv         Zeitraum: 21082017_18092017

```{r, echo = FALSE}
# ====================== R e a d   d a t a ============== alternative
library(writexl)
if (Read_Fresh_from_File)  {
  print("reading file.... ")
  # --- read excel ----
  if (endung == ".xlsx")   {
     rawframe <- read_xlsx (path = paste0(rel_fpath, filename, endung ), guess_max=100000)
     warnings()
  } else
  {  #--- read other formats ---
  rawframe <- read_data_csv(data_file_name = paste0(rel_fpath, filename, endung))
  rawframe2 <- read_data_csv(data_file_name = paste0(rel_fpath, filename2, endung))
  rawframe3 <- read_data_csv(data_file_name = paste0(rel_fpath, filename3, endung))
  rawframe4 <- read_data_csv(data_file_name = paste0(rel_fpath, filename4, endung))
  rawframe5 <- read_data_csv(data_file_name = paste0(rel_fpath, filename5, endung))
  rawframe6 <- read_data_csv(data_file_name = paste0(rel_fpath, filename6, endung))
  rawframe7 <- read_data_csv(data_file_name = paste0(rel_fpath, filename7, endung))
  rawframe8 <- read_data_csv(data_file_name = paste0(rel_fpath, filename8, endung))
  }

  saveRDS(rawframe, file = paste(rel_fpath_output , filename, ".rData"))
  saveRDS(rawframe2, file = paste(rel_fpath_output , filename2, ".rData2"))
  saveRDS(rawframe3, file = paste(rel_fpath_output , filename3, ".rData3"))
  saveRDS(rawframe4, file = paste(rel_fpath_output , filename4, ".rData4"))
  saveRDS(rawframe5, file = paste(rel_fpath_output , filename5, ".rData5"))
  saveRDS(rawframe6, file = paste(rel_fpath_output , filename6, ".rData6"))
  saveRDS(rawframe7, file = paste(rel_fpath_output , filename7, ".rData7"))
  saveRDS(rawframe8, file = paste(rel_fpath_output , filename8, ".rData8"))
  
  print("...Data saved in RData file")
} else { # Read_Fresh_from_File gleich FALSE
  rawframe <- readRDS(file = paste(rel_fpath_output , filename, ".rData"))
  rawframe2 <- readRDS(file = paste(rel_fpath_output , filename2, ".rData2"))
  rawframe3 <- readRDS(file = paste(rel_fpath_output , filename3, ".rData3"))
  rawframe4 <- readRDS(file = paste(rel_fpath_output , filename4, ".rData4"))
  rawframe5 <- readRDS(file = paste(rel_fpath_output , filename5, ".rData5"))
  rawframe6 <- readRDS(file = paste(rel_fpath_output , filename6, ".rData6"))
  rawframe7 <- readRDS(file = paste(rel_fpath_output , filename7, ".rData7"))
  rawframe8 <- readRDS(file = paste(rel_fpath_output , filename8, ".rData8"))
}
#--- create / read data control file ---
if (file.exists(path = paste0(rel_fpath, filename, '_data_control.xlsx')))
  data_control_file <-  paste0(rel_fpath, filename, '_data_control.xlsx') else
    data_control_file <- paste0(rel_fpath, 'data_control_template.xlsx')
datadesc <- read_datadesc(data_control_file)
  
# initial exclusion of not relevant variables according to Data Description entry in column Clean Data Frame
rawframe <- rawframe[ ,datadesc$`Clean Data Frame`!="exclude"]  
rawframe2 <- rawframe2[ ,datadesc$`Clean Data Frame`!="exclude"]  
rawframe3 <- rawframe3[ ,datadesc$`Clean Data Frame`!="exclude"]  
rawframe4 <- rawframe4[ ,datadesc$`Clean Data Frame`!="exclude"]  
rawframe5 <- rawframe5[ ,datadesc$`Clean Data Frame`!="exclude"]  
rawframe6 <- rawframe6[ ,datadesc$`Clean Data Frame`!="exclude"]  
rawframe7 <- rawframe7[ ,datadesc$`Clean Data Frame`!="exclude"]
rawframe8 <- rawframe8[ ,datadesc$`Clean Data Frame`!="exclude"]  

# write control file if not present
if (!file.exists(path = paste0(rel_fpath, filename, '_data_control.xlsx')))  {
  datadesc[1:length(names(rawframe)), 1] <-  names(rawframe)
  writexl::write_xlsx(datadesc, path = paste0(rel_fpath, filename, '_data_control.xlsx'))
}
```

Remove duplicate measurements, generate one combined rawframe for ICT measurements
```{r, echo = FALSE}
# check for double measurements - 258 cycles of two A5E02758387 - files
cat("check for double measurements - duplicated measurement cycles (SFILENR) of two A5E02758387 - files: ", 
sum(duplicated(c(unique(rawframe$SFILENAME), unique(rawframe2$SFILENAME), unique(rawframe3$SFILENAME), unique(rawframe4$SFILENAME), unique(rawframe5$SFILENAME), unique(rawframe6$SFILENAME), unique(rawframe7$SFILENAME)))) 
, "\n")

#combine to one huge frame
rawframe <- rawframe %>% rbind(rawframe2) %>% rbind(rawframe3) %>% rbind(rawframe4) %>% rbind(rawframe5) %>% rbind(rawframe6) %>% rbind(rawframe7) %>% rbind(rawframe8) 
remove(rawframe2, rawframe3, rawframe4, rawframe5, rawframe6, rawframe7, rawframe8)
# remove duplicated measurements
cat("removed duplicated single measurements (NTDID): ", 
sum(duplicated(rawframe$NTDID)) # 202976
, "\n")
rawframe <- rawframe[duplicated(rawframe$NTDID)=="FALSE", ]

#sorting - eases further exploration
rawframe <- rawframe %>% arrange(SPLATZNR, DTDATUM, NORDER)

#basic transformations - datenspezifisch!
if (basic_tranform) {
  rawframe <- rawframe[rawframe$SEINHEIT != 'OE' ,]
}

cat("\nIn total", nrow(rawframe), "single measurement points (NTDID) after removing Soll-Istwerte lines\n\n")  
```


###2.1.2 Collect Q Data

Reading Q Data DPMI_Fehler_alle_alle_201606_bis_201608_31.07.2018_adapt.csv (manual merge of two deliveries).

Remove duplicates.
Restrict to single row per FID.
Integrate agreed error mapping into Q Data.

```{r, echo = FALSE, warning=FALSE}
q_daten <- read_delim("data/DPMI_Fehler_alle_alle_201606_bis_201608_31.07.2018_adapt.csv", 
    ";", escape_double = FALSE, col_types = cols(DATUM = col_datetime(format = "%d.%m.%Y %H:%M")),
    locale = locale(encoding = "ISO-8859-1"), 
    trim_ws = TRUE)
# remove Prüfstufe Gerät, subsequently delete empty columns
#q_daten <- q_daten[q_daten$PRUEFSTUFE != 'G', ]
#q_daten <- q_daten[, 1:15]

# remove duplicates in Q Data: 
cat("remove duplicates in Q Data: ", sum(duplicated(q_daten)), "rows \n")  
q_daten <- q_daten[duplicated(q_daten)=="FALSE", ]

# remove double entries of FID - first approach - of course information is lost - future: provide at least removed marker or move up as additional features
q_daten <- q_daten[duplicated(q_daten$FID)=="FALSE", ]
cat(nrow(q_daten), "rows remaining\n")  

# Integrate agreed error mapping into Q Data
Error_mapping <- data.frame(BEFUND = c("B000", "B100", "V110", "V120", "V210", "V230"),
  BEFUND_clean = c("Pseudo Error", "Offene Lötstelle", "Offene Lötstelle", "Falsches Bauteil", "Offene Lötstelle", "Kurzschluß"))

q_daten <- left_join(q_daten, Error_mapping)

```
###2.1.3 Collect Localisation info

Parse localisation info from:
A5E02758365A-02.pcf  for A5E02758387
A5E31281435-AC.pcf for A5E31281377
A5E01283404A-02.pcf for A5E32692783 AND A5E01283425
A5E02144933A-02.pcf for A5E33212840

```{r, echo = FALSE, message=FALSE, warning=FALSE}
locinfo <- extract_location("data/A5E01283404A-02.pcf")   # #A5E01283425
locinfo5 <- locinfo # same loc info for both boards
locinfo$SARTIKELNR <- "A5E01283425"
locinfo <- select(locinfo, SARTIKELNR, everything())

locinfo2 <- extract_location("data/A5E02758365A-02.pcf")   # #A5E02758387
locinfo2$SARTIKELNR <- "A5E02758387"
locinfo2 <- select(locinfo2, SARTIKELNR, everything())

locinfo4 <- extract_location("data/A5E31281435-AC.pcf")   # #A5E31281377
locinfo4$SARTIKELNR <- "A5E31281377"
locinfo4 <- select(locinfo4, SARTIKELNR, everything())

# see above locinfo5   # #A5E32692783
locinfo5$SARTIKELNR <- "A5E32692783"
locinfo5 <- select(locinfo5, SARTIKELNR, everything())

locinfo6 <- extract_location("data/A5E02144933A-02.pcf")   # #A5E33212840
locinfo6$SARTIKELNR <- "A5E33212840"
locinfo6 <- select(locinfo6, SARTIKELNR, everything())

locinfo <- locinfo %>% bind_rows(locinfo2) %>% bind_rows(locinfo4) %>% bind_rows(locinfo5) %>% bind_rows(locinfo6)
remove(locinfo2, locinfo4, locinfo5, locinfo6)

locinfo <- locinfo[is.na(as.numeric(locinfo$SMMNR_clean)), ] # nicht so schön, weil er NAs introduced by coercion ausnützt

```

Extraction of SMMNR from ICT measurement data. 
Following charts show the position of the components per variant.
Note: board orientation and scale is not shown/adapted due to missing information

```{r, echo = FALSE, message=FALSE, warning=FALSE}
SMMNR_table <- rawframe %>% select(SARTIKELNR, SCOMB_PL, SMMNR, SEINHEIT) %>% unique()  # DTU DTO reinnhemne erhöht die verschiedene entries uaf 6tsd..
SMMNR_table$SMMNR_clean <- SMMNR_table$SMMNR

for(i in 1:length(SMMNR_table$SMMNR_clean)) {
  underscore1 <- regexpr("_", SMMNR_table$SMMNR_clean[i]) # position of first "_"
  underscore2 <- regexpr("_", substring(SMMNR_table$SMMNR_clean[i], underscore1+1, nchar(SMMNR_table$SMMNR_clean[i]))) # position of second "_"
  if ((underscore1 > 0) & (underscore2 > 0)) {
    SMMNR_table$SMMNR_clean[i] <- substring(SMMNR_table$SMMNR_clean[i], underscore1+1, underscore1+underscore2-1) # delete string before first "_"
  }
  underscore <- regexpr("_", SMMNR_table$SMMNR_clean[i]) # position of next "_"
  if (underscore > 0)  {
    SMMNR_table$SMMNR_clean[i] <- substring(SMMNR_table$SMMNR_clean[i], 1, underscore-1) #cut string from next "_"
  }
  hyphen <- regexpr("-", SMMNR_table$SMMNR_clean[i]) # position of next "-"  
  if (hyphen > 0)  {
    SMMNR_table$SMMNR_clean[i] <- substring(SMMNR_table$SMMNR_clean[i], 1, hyphen-1) #cut string from next "-"
    }  
}

SMMNR_table <- left_join (SMMNR_table, locinfo)

plot(SMMNR_table$X_loc, SMMNR_table$Y_loc)
write.table(SMMNR_table,       # write resulting data into CSV file with tabstops
            file=paste0(rel_fpath, "SMMNR_loc_info", endung),
            sep=";", quote=FALSE, row.names=FALSE, dec=",") 
ggplot(SMMNR_table) +
  geom_point(mapping = aes(x = X_loc, y = Y_loc), size=0.5) +
  facet_wrap(~SARTIKELNR, ncol = 2, scales = "free")+
         ggtitle("Localisation Info per board variant")

```


##2.3 Explore Data and Visualization

Routines:  
Create sub dataframes of numeric and non numeric variables  
Do basic data transformation and sorting  
```{r, echo = FALSE}
#create a sub dataframe of numeric varialbes
df_num <- as.data.frame (rawframe [ , sapply(rawframe,is.numeric)])
print (paste("Dimension of dataframe of numeric variables (columns)", dim(df_num)[2]))

#create a sub dataframe of non numeric varialbes
df_not_num <- as.data.frame (rawframe [ , !sapply(rawframe,is.numeric)])
print (paste("Dimension of dataframe of not-numeric variables (columns)", dim(df_not_num)[2], ":"))
cat (names(df_not_num), "\n")
```

### 2.3.1 Boxplots and Histograms of all numerical variables

This section shows boxplots, histograms and the summary statistics of all numerical variables as well as related comments and descriptions which are extracted from the data control file.

``{r, echo = FALSE}
if (printpdf) pdf(outputfile)
if (scatterplot) plot (df_num) # scatter plot
plot_num (df_num, datadesc)
if (printpdf) dev.off()    # reset device to screen plot 
```



### 2.3.2 Exploration of all non numerical variables

This section shows histograms and summary statistics of all non numerical variables as well as related comments and descriptions which are extracted from the data control file.

``{r, echo = FALSE}
summary_non_num (df_not_num, datadesc)
```

## 2.4 Verify data quality

### 2.4.1 check missing values
Following number of NA and percentage of NA were discovered in the data set

``{r, echo = FALSE}
j <- 1 #j is rawframe index, i is datadesc index
for(i in 1:nrow(datadesc)){  
  if (datadesc$`Clean Data Frame`[i] !='exclude') {
    cat(paste(names(rawframe[j]), "\t  # of NA:", colSums(is.na(rawframe[j])), 
              "  in %", round(colSums(is.na(rawframe[j])/nrow(rawframe)), 3)*100), "\n")
    datadesc$`% of na values`[i] <- round(colSums(is.na(rawframe[j]))/nrow(rawframe), 3)*100
    j <- j+1
  }  
}    
#write.csv2(datadesc$`% of na values`, file = "na_%.csv")
writexl::write_xlsx(datadesc, path = paste0(rel_fpath, filename, '_data_control.xlsx'))

```


### 2.4.2 check collinearity

Correlation between input variables in ICT measurements is displayed: 

``{r, echo = FALSE}
# produce correlation matrix
check_collinearity (df_num)
```


# 3. Exploration 
## 3.1 Feature Engineering on rawframe level   

Routines:  
Add index overall and relative index for one measurement cycle (SFILENR) according to NORDER sorting
Add violation id = relative index when threshold violated

NORDER nr. has gaps (dependent on test program)
NTDID has also gaps independent from NORDER, can differ from FID to FID

```{r, echo = FALSE}
rawframe$Ix <- 1:nrow(rawframe)

#rawframe$Ix_rel <- 0
#A5E02758387 Isolation Test and Connection Test leads to violation because DTU and DTO are 0.0 while DMW ist constantly 1.0
#--> eliminate those otherwise the 2 delivered files do not fit together
SFILENR_help <- rawframe %>% filter(SMMNR!='IsolationTest', SMMNR!='ConnectionTest') %>%  group_by(SFILENAME) %>% summarize(SARTIKELNR=unique(SARTIKELNR), Ix_rel = min(Ix))
rawframe <- left_join(rawframe, SFILENR_help, by = c("SFILENAME", "SARTIKELNR"))
remove(SFILENR_help)
rawframe$Ix_rel <- rawframe$Ix - rawframe$Ix_rel+1

rawframe$viol_id <- NA
#for (i in 1:nrow(rawframe)) {
#  if ((rawframe$DMW[i]>rawframe$DTO[i])|(rawframe$DMW[i]<rawframe$DTU[i])) {
#    rawframe$viol_id[i] <- rawframe$Ix_rel[i]
#    }
#    if (i %% 10000 == 0) {print (i)}
#}
# loop wir ab 3 Mio. Interationen äußerst langsam!! insgesamte Dauer über 1/2 Stunde

# remove dashes in SMMNR text
rawframe$SMMNR <- str_replace_all(rawframe$SMMNR, "([[:punct:]])|\\s+", "_")

#saveRDS(rawframe, file = paste(rel_fpath_output, "rawframe.rData"))
```
```{r, echo = FALSE}

rawframe <- readRDS(file = paste(rel_fpath_output , "rawframe.rData")) # to shorten process before

```



## 3.2 Pivot Section - view per FID / Board

Generation of FID / board related view:

- overall count of single measurements per board
- time(s) of passing measurement cycle, first/last pass
- time(s) of failing measurement cycle, first/last fail
- nr. of threshold violations

Following table is the base for further exploration after combination with Q-Data

```{r, echo = FALSE, message=FALSE, warning=FALSE}
# grouping by SID, showing pass and failure times with count of violations
SID_table <- rawframe %>% group_by(SID) %>% summarize(count=n(), SARTIKELNR=unique(SARTIKELNR)) 

SID_table_pass <- rawframe %>% filter(SERGEBNIS=='PASS') %>% group_by(SID) %>% 
  summarize(count_pass=n(), time_pass=list(as.character(unique(DTDATUM))),
            firstpass_dt=min(DTDATUM), lastpass_dt=max(DTDATUM)
            )
SID_table_fail <- rawframe %>% filter(SERGEBNIS=='FAIL') %>% group_by(SID) %>% 
  summarize(count_fail=n(), time_fail=list(as.character(unique(DTDATUM))),
            cycle_fail=length(unique(DTDATUM)),
            firstfail_dt=min(DTDATUM), lastfail_dt=max(DTDATUM),
            limit_viol=sum((DMW>DTO)|(DMW<DTU)))
SID_table <- SID_table %>% full_join (SID_table_pass) %>% full_join (SID_table_fail)
remove(SID_table_pass, SID_table_fail)
# remove last_.. time when only one occurence
SID_table$lastpass_dt[SID_table$firstpass_dt==SID_table$lastpass_dt] <- NA
SID_table$lastfail_dt[SID_table$firstfail_dt==SID_table$lastfail_dt] <- NA
head(SID_table)
```

## 3.3 Combine Q-Daten

several joins of Q Data, pseudo error and label qualification from Q Data.
Label qualifiation means those measurements where ICT measurement is before a repair entry.

```{r, echo = FALSE, results='hold'}
#SID_table_full <- SID_table %>% full_join (q_daten, by = c("SID" = "FID")) 
SID_tableinner <- SID_table %>% inner_join (q_daten, by = c("SID" = "FID")) 
SID_table <- SID_table %>% left_join (q_daten, by = c("SID" = "FID")) 

# fail vs repair time
#SID_tableinner$fail_repair <- 0
#for (i in 1:nrow(SID_tableinner)) {
#  SID_tableinner$fail_repair[i] <- paste(SID_tableinner$time_fail[[i]][[1]],SID_tableinner$DATUM[i])
#}

# true label flag
#SID_tableinner$label_indic <- SID_tableinner$DATUM > SID_tableinner$firstfail_dt
#SID_table_full$label_indic <- SID_table_full$DATUM > SID_table_full$firstfail_dt
SID_table$label_indic <- SID_table$DATUM > SID_table$firstfail_dt
cat("RESULTS:\nFID / boards of ICT measurements which have an entry in Q Data: ", nrow(SID_tableinner), "\n")  
cat("FID / boards where repair is after ICT measurement (error detected in ICT and afterwards repaired): ", sum(SID_table$label_indic, na.rm = TRUE), "\n")  

#SID_table_full$pseudo <- !is.na(SID_table_full$firstfail_dt) & !is.na(SID_table_full$firstpass_dt)  # 231 pseudo errors
# pseudo label: es gibt mindestens einen fail cycle und mindestens ein pass cycle und kein real error vorhanden
SID_table$pseudo <- !is.na(SID_table$firstfail_dt) & !is.na(SID_table$firstpass_dt) & is.na(SID_table$BEFUND_clean)  # 256 pseudo errors
cat("Nr. of pseudo errors detected: ", sum(SID_table$pseudo), "\n")  

# remove empty columns after left_join
SID_table <- select(SID_table, -c(STATUS, BEFUND_FBG, ZNR_BE, BE_KURZTEXT, BKZ_BE, PRUEFER1))
```

How many FID - pure fails are in Q Data? 
Following tables show 'pure' fails dependent on board variants and dependent on type of Prüfstufe

```{r, echo = FALSE, results='hold'}
cat("distribution of fails in Q Data over variants/pruefstufen\n")  
pure_fail_Q_variant <- q_daten %>% group_by(OBERSTUFE) %>% summarize(count=n())
print(pure_fail_Q_variant)
pure_fail_Q_pruefst <- q_daten %>% group_by(PRUEFSTUFE) %>% summarize(count=n())
print(pure_fail_Q_pruefst)
```


How are the FID - pure fails distributed over the variants in ICT measurement Data?

```{r, echo = FALSE, results='hold'}
# only fails: 6490 - most of them from Q data  (6475 - 32 innerjoin + 47 fail related ICT)
# sum(is.na(SID_table_full$firstpass_dt))

cat("pure fails related to FID in ICT data: ",
  sum(is.na(SID_table$firstpass_dt))
, "\n")
pure_fail_ICT <- SID_table %>% filter(is.na(firstpass_dt)) %>% group_by(SARTIKELNR) %>% summarize(count=n())
cat("distribution of fails in Q Data over variants\n") 
print(pure_fail_ICT)

# provide pure fail flag
SID_table$pure_fail <- is.na(SID_table$firstpass_dt)   
```


Number of pseudo errors per variant

```{r, echo = FALSE, results='hold'}
count_pseudo <- SID_table %>% filter(pseudo==TRUE) %>% group_by(SARTIKELNR) %>% summarize(count=n())
print(count_pseudo)
ggplot(data = SID_table, mapping = aes(x = pseudo)) +  
         geom_bar()+#aes(fill = PRUEFSTUFE)) +
         facet_wrap(~ SARTIKELNR, ncol = 2, scales = "free")+
         ggtitle("Distribution of Pseudo errors across the variants")

```
There are no repair cases for pseudo errors --> repair path not burdened with pseudo errors

## 3.4 Pivot Section Measurement cycle view - SFILENAME

Generation of measurement cycle/ SFILENAME related view:
- overall count of single measurements per board
- time of passing measurement cycle
- time of failing measurement cycle
- nr. of threshold violations per cycle

```{r, echo = FALSE, results='hold', message=FALSE, warning=FALSE}
SFILENR_table <- rawframe %>% group_by(SFILENAME) %>% summarize(count=n(), SARTIKELNR=unique(SARTIKELNR), SID=unique(SID),
                                                                DTDATUM=unique(DTDATUM))
SFILENR_table <- select(SFILENR_table, SARTIKELNR, SID, everything())

SFILENR_table_pass <- rawframe %>% filter(SERGEBNIS=='PASS') %>% group_by(SFILENAME) %>% 
  summarize(count_pass=n(), time_pass=(unique(DTDATUM)))

SFILENR_table_fail <- rawframe %>% filter(SERGEBNIS=='FAIL') %>% group_by(SFILENAME) %>% 
  summarize(count_fail=n(), time_fail=(unique(DTDATUM)), limit_viol=sum((DMW>DTO)|(DMW<DTU)))

SFILENR_table <- SFILENR_table %>% full_join (SFILENR_table_pass) %>% full_join (SFILENR_table_fail)
remove(SFILENR_table_pass, SFILENR_table_fail)
#head(SFILENR_table)

# Verteilung der Abbrüche bzw. pass
#SFILENR_subtable <- filter(SFILENR_table, SARTIKELNR=='A5E02758387')
#hist(SFILENR_subtable$count, breaks = 100)
hist(SFILENR_table$count, breaks = 100)

ggplot(data = SFILENR_table, mapping = aes(x = count)) +  
         geom_histogram(binwidth = 5) +
#         coord_cartesian(xlim = c(0, 1000)) +
         facet_wrap(~ SARTIKELNR, ncol = 2, scales = "free") +
         ggtitle("Nr. of measurement counts")

ggplot(data = SFILENR_table, mapping = aes(x = count_fail)) +  
         geom_histogram(binwidth = 5) +
#         coord_cartesian(xlim = c(0, 1000)) +
         facet_wrap(~ SARTIKELNR, ncol = 2, scales = "free_y") +
         ggtitle("Nr. of measurement counts when failed") 

ggplot(data = SFILENR_table, mapping = aes(x = count_pass)) +  
         geom_histogram(binwidth = 10) +
#         coord_cartesian(xlim = c(0, 1000)) +
         facet_wrap(~ SARTIKELNR, ncol = 2, scales = "free_y") +
         ggtitle("Nr. of measurement counts when passed")

#SFILENR_subtable <- filter(SFILENR_table, SARTIKELNR=='A5E02758387') %>% arrange(desc(count))

```
Histogram of SFILENR_table$count:
Two variants (A5E02758387, A5E33212840) show PASS measurement cycles (SFILENR) with doubled entries. What is the reason for it?
--> working assumption for analytics: just keep one successful data set

Facetwrap Nr. of measurement counts when failed:
At COMET ICT measurements are stopped when threshold is violated (SARTIKELNR A5E01283425 and A5E32692783)
Other variants complete measurements also when some fail (better for model building)

Facetwrap Nr. of measurement counts when passed:
A5E01283425 shows nicht ganzzahlige vielfache von one measurement cycles measurements (148, 751) which lead to a PASS. Any reason?
--> working assumption for analytics: remove fractional PASS


Distribution of threshold violations per variant:

```{r, echo = FALSE, results='hold', message=FALSE, warning=FALSE}
# viol_id distribution
ggplot(data = rawframe, mapping = aes(x = viol_id)) +  
         geom_histogram(binwidth = 1) +
#         coord_cartesian(xlim = c(0, 1000)) +
         facet_wrap(~ SARTIKELNR, ncol = 2, scales = "free")+
         ggtitle("Distribution of threshold violations per variant") 

ggplot(data = filter(rawframe, SARTIKELNR=='A5E02758387', SMMNR!='IsolationTest', SMMNR!='ConnectionTest', SID != 'T-J86414591'), mapping = aes(x = viol_id)) +  
         geom_histogram(binwidth = 1) +
         ggtitle("Distribution of threshold violations for A5E02758387") 

rawframe_sub <- rawframe %>% filter(SARTIKELNR=='A5E02758387', SMMNR!='IsolationTest', SMMNR!='ConnectionTest', !is.na(viol_id), SID != 'T-J86414591') %>% group_by (SMMNR, viol_id, SID) %>% count() %>% arrange(desc(n))
head(rawframe_sub,50)

```         
A5E02758387 Isolation Test and Connection Test leads to violation because DTU and DTO are 0.0 while DMW ist constantly 1.0
--> eliminate those
Nevertheless leads to PASS!?  
--> working assumption for analytics: remove those measurements (one SCOMB_PL)
Board T-J86414591 shows > 400 violations
--> eliminate T-J86414591

Trend of theshold violations per variant:

```{r, echo = FALSE, results='hold', message=FALSE, warning=FALSE}
#  geom_point(size=2, shape=23)
ggplot(data = SFILENR_table, mapping = aes(x = time_fail, y = limit_viol)) +  
         geom_point() +
         facet_wrap(~ SARTIKELNR, ncol = 2, scales = "free")+
         ggtitle("Trend of theshold violations per variant") 

ggplot(data = filter(SFILENR_table, SARTIKELNR == 'A5E02758387', limit_viol < 100 ), mapping = aes(x = time_fail, y = limit_viol)) +  
         geom_point() +
         ggtitle("Trend of theshold violations for A5E02758387") 

#rawframe_sub <- rawframe %>% filter(SARTIKELNR=='A5E02758387', SMMNR!='IsolationTest', SMMNR!='ConnectionTest') %>% arrange(desc(viol_id))

#SFILENR_subtable <- SFILENR_table %>% filter(SARTIKELNR=='A5E02758387', SID=='T-J86414591') %>% arrange(desc(limit_viol))
```         

Trend of theshold violations per variant facet_wrap:
Outlier caused by two A5E33212840 boards: T-H76000335, T-H66267724 (one pseudo, one pure fail/no repair info)
Outlier caused by A5E02758387 board: T-J8641459 (pseudo, first measurement cycle was aborted)
- specific background? (most of the values 0,000000000000 or 4.294.967.295,0 - measurement value range of equipment?)
--> working assumption for analytics: remove outlier

## 3.5 Save Result Tables 

Above shown result tables will be saved - extended for Tableau input in future sprints.

```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
print("...save board data frame")
outputcsv <- "./output/board.csv"
write.table(select(SID_table, -(time_pass:time_fail)),   # write resulting data into CSV file with tabstops without embedded date lists
            file=outputcsv,
            sep=",", quote=FALSE, row.names=FALSE, dec=".") 
```

## 3.6 Violation on the board

Note: board orientation and scale is not shown/adapted due to missing information

```{r, echo = FALSE, message=FALSE, warning=FALSE}
messw_fail_loc <- rawframe %>% filter(!is.na(viol_id)) %>% group_by(SCOMB_PL) %>% summarize(count_viol = n())

SMMNR_table <- left_join (SMMNR_table, messw_fail_loc)
remove(messw_fail_loc)
ggplot(SMMNR_table) +
  geom_point(mapping = aes(x = X_loc, y = Y_loc, colour = count_viol), size=0.8) +
#  scale_fill_gradientn(colours=c("black","yellow","red"), breaks=c(0,30,50), labels=format(c("0","30","50")) ) +
  facet_wrap(~SARTIKELNR, ncol = 2, scales = "free")+
         ggtitle("Threshold violations on the board")
```

Violation distribution on the board - Facetwrap:
A5E32692783 component ICT_COMET001A5E32692783170.170.2_Voltage_21 with 830 violations! - Reason:
DMW   DTU  DTO
-45    0   -45

Nevertheless leads to PASS!?  
--> working assumption for analytics: remove those measurements (one SCOMB_PL)

--> general impact for analytics: location info will be input variable for model building

## 3.7 Trend of measurement cycle failures over the time

```{r, echo = FALSE, results='hold'}
messw_fail_trend <- rawframe %>% filter(SERGEBNIS=='FAIL') %>% group_by(SFILENAME) %>% summarize(count_fail=n(),
                                                                                           time_fail=unique(DTDATUM), SARTIKELNR = unique(SARTIKELNR))
ggplot(data = messw_fail_trend, mapping = aes(x = time_fail)) +
         geom_histogram(binwidth = 10000)+
  facet_wrap(~SARTIKELNR, ncol = 2, scales = "free")+
         ggtitle("Trend of measurement cycle failures per variant")

# repeat plot for A5E02758387 with different binwidth
ggplot(data = filter(messw_fail_trend, SARTIKELNR=='A5E02758387'), mapping = aes(x = time_fail)) +
         geom_histogram(binwidth = 100000)+
  #facet_wrap(~SARTIKELNR, ncol = 2, scales = "free")+
         ggtitle("Trend of measurement cycle failures A5E02758387")
```

Trend diagrams of measurement cycle failures how strong peaks with high number of failures. 
--> Data analytics need to examine whether this is due to variation in tested boards or dependent on external influence (manufacturing conditions) 

## 3.8 SMMNR Consideration - Varco

Varco hypothesis: degree of pollution of a single component can be explained by the coefficient of variation (varco)

When calculating the varco only components measurements must be considered where the component was not responsible for the fail of the board!

```{r, echo = FALSE, warning=FALSE}
#Varco Berechnung pro Bauteil, variantenbezogen, ohne ursächliche 
board_view <- rawframe %>% filter(is.na(viol_id)) %>% group_by(SARTIKELNR, SMMNR, SERGEBNIS) %>% summarize(count=n(), DMW_mean=mean(DMW), DMW_sd=sd(DMW), Varco=DMW_sd/DMW_mean)
# limit_viol=sum((DMW>DTO)|(DMW<DTU)) macht keinen Sinn, da ausgefiltert 
#%>% arrange(desc(limit_viol))

#calculate Varco PASS/FAIL ratio - 1 
board_view$varco_deviation <- 0
for (i in 1:nrow(board_view)) {
  if (board_view$SERGEBNIS[i]=="FAIL") {
    board_view$varco_deviation[i] = abs(board_view$Varco[i] / board_view$Varco [i+1] - 1)
  }
  else {
    board_view$varco_deviation[i] = abs(board_view$Varco[i-1] / board_view$Varco [i] - 1)
  }
}

board_view$varco_deviation[is.na(board_view$varco_deviation)] <- 0 # in case of 0 deviation

board_view <- board_view %>% arrange(desc(varco_deviation))

board_view$row_nr <- 0
for (i in 1:nrow(board_view)) {
    board_view$row_nr[i] <- i
}

```
List of statistic values DMW mean, DMW standard error, coefficient of variation (varco) and varco ratio for FAIL/PASS per component
a) accross all ARTIKELNR (first 50)
b) per ARTIKELNR (first 50)
```{r, echo = FALSE}
#Visualize Varco wrt Pass and Fail
board_view %>% select(-c(count, row_nr)) %>% top_n(50, varco_deviation)
board_view %>% select(-c(count, row_nr)) %>% filter(SARTIKELNR=="A5E01283425") %>% top_n(50, varco_deviation)
board_view %>% select(-c(count, row_nr)) %>% filter(SARTIKELNR=="A5E32692783") %>% top_n(50, varco_deviation)
board_view %>% select(-c(count, row_nr)) %>% filter(SARTIKELNR=="A5E02758387") %>% top_n(50, varco_deviation)
board_view %>% select(-c(count, row_nr)) %>% filter(SARTIKELNR=="A5E32692783") %>% top_n(50, varco_deviation)
board_view %>% select(-c(count, row_nr)) %>% filter(SARTIKELNR=="A5E33212840") %>% top_n(50, varco_deviation)

```
Plots of varco deviation for a significant component C705 at A5E02758387
A) DMW variation in time per PASS, FAIL
B) DMW histogramm PASS, FAIL
```{r, echo = FALSE}
#selektiertes Bauteil
spec_bauteil <- rawframe %>% filter(SMMNR=='C705', SARTIKELNR=="A5E02758387") %>% arrange(DTDATUM)

#Messwerte im Zeitvertlauf f?r PASS
ggplot(data = spec_bauteil[spec_bauteil$SERGEBNIS=="PASS",], aes(x = DTDATUM)) +
          geom_line(aes(y = DMW)) +
          geom_line(aes(y = DTU), colour="red") +
          geom_line(aes(y = DTO), colour="red")+
          coord_cartesian(ylim = c(0.0000000009, 0.0000000014))+
          ggtitle ("trend for C705 on A5E02758387 - PASS cycles")

#Messwerte im Zeitvertlauf f?r FAIL
ggplot(data = spec_bauteil[spec_bauteil$SERGEBNIS=="FAIL",], aes(x = DTDATUM)) +
          geom_line(aes(y = DMW)) +
          geom_line(aes(y = DTU), colour="red") +
          geom_line(aes(y = DTO), colour="red")+
          coord_cartesian(ylim = c(0.0000000009, 0.0000000014))+
          ggtitle ("trend for C705 on A5E02758387 - FAIL cycles")

#Normierung des Messwertintervalls [DTU, DTO] auf [-1;1]
spec_bauteil$DMW_norm <- (spec_bauteil$DMW - spec_bauteil$DTU)/(spec_bauteil$DTO - spec_bauteil$DTU)*2 -1

#Messwertverteilung f?r PASS
ggplot(data = spec_bauteil[spec_bauteil$SERGEBNIS=="PASS",], mapping = aes(x = DMW_norm)) +  
         geom_histogram(binwidth = 0.05) +
         coord_cartesian(xlim = c(-2, 2))+
          ggtitle ("distribution for C705 on A5E02758387 - PASS cycles")

#Messwertverteilung f?r FAIL
ggplot(data = spec_bauteil[spec_bauteil$SERGEBNIS=="FAIL",], mapping = aes(x = DMW_norm)) +  
         geom_histogram(binwidth = 0.05) +
         coord_cartesian(xlim = c(-2, 2))+
          ggtitle ("distribution for C705 on A5E02758387 - FAIL cycles")

top50_varco_deviation <- board_view [1:100,] %>% filter(SARTIKELNR=="A5E02758387") %>% top_n(100, varco_deviation) %>% select(SMMNR, varco_deviation)
SMMNR_subtable <- SMMNR_table %>% filter(SARTIKELNR=="A5E02758387") %>% filter (SMMNR %in%  top50_varco_deviation$SMMNR)
ggplot(data = SMMNR_subtable) +
  geom_point(mapping = aes(x = X_loc, y = Y_loc, colour = count_viol), size=0.8) +
#  scale_fill_gradientn(colours=c("black","yellow","red"), breaks=c(0,30,50), labels=format(c("0","30","50")) ) +
         ggtitle("Location of top 50 varco_deviation components on A5E02758387")
```



Distribution of varco deviation accross all components orderd by magnitude
```{r, echo = FALSE}
#Verteilung der Varco PASS/FAIL difference nach Baugruppen absteigend
ggplot(data = board_view, aes(x = row_nr)) +
          geom_line(aes(y = varco_deviation)) +
          ggtitle ("Varco deviation accross all components")

#Auswahl Baugruppe A5E02758387
board_view <- board_view[board_view$SARTIKELNR=="A5E02758387",]
board_view$row_nr <- 0
for (i in 1:nrow(board_view)) {
    board_view$row_nr[i] <- i
}
ggplot(data = board_view, aes(x = row_nr)) +
          geom_line(aes(y = varco_deviation)) +
          coord_cartesian(ylim = c(5,0)) +
          ggtitle ("Varco deviation accross all comp. A5E02758387, y-axis fixed [0;5]")
```

Distribution of varco deviation accross all components:
--> Data analytics will have to focus on those components which show high variation in the FAIL measurement cycles.


# 4 Data preparation and modeling

# 4.1 Individual measurements as features  

Remove doubled PASS measurements
Remove SMMNR "Isolation Test" and "Connection Test"
```{r, echo = FALSE}
# restrict on A5E02758387
rawframe87 <- rawframe[rawframe$SARTIKELNR=="A5E02758387", ]
# for A5E02758387 only 2 SID with double measurements PASS - taken from SFILENR_table below: 
# T-H76109604 SamAD390650PC180720160650280.erg
# T-H76109585 SamAD390650PC180720160700000.erg - duplicates characterized by SFILENAME and NORDER
# Eliminierung der SMMNR für "ConnectionTest" und "IsolationTest" müsste möglicherweise schon früher bei 3.1 erfolgen 
rawframe87 <- rawframe87 %>% distinct(SFILENAME, NORDER,  .keep_all = TRUE) %>% filter (SMMNR != "ConnectionTest", SMMNR != "IsolationTest", SID != 'T-J86414591')

saveRDS(rawframe87, file = paste(rel_fpath_output, "A5E02758387.rData87"))
```

Read rawframe for A5E02758387 (focus for analytical models)     
```{r, echo = FALSE}
rawframe87 <- readRDS(file = paste(rel_fpath_output , "A5E02758387.rData87")) # to shorten process before
```

Transpose so each measurement is a predictor --> 753 predictors   
```{r, echo = FALSE, message=FALSE, warning=FALSE}
rawframe_87small <- data.frame(select(rawframe87, -c(NTDID, SEINHEIT, DTU, DTO, NORDER, SCOMB_PL, Ix, Ix_rel, viol_id))) # Ausschlüsse sind spezifisch für die Messung - würden weitere features erzeugen
# eine Zeile bezieht sich auf einen Messzyklus SFILENAME
frame87_transpose <- rawframe_87small %>% spread(SMMNR, DMW) # spread auf Basis von SFILENAME, 824 einträge für A5E02758387
# !! spread macht SMMNR Einträge zu col names. Bindestriche in col names machen Probleme/erschweren model funktionen:
# https://stackoverflow.com/questions/48651370/dash-in-column-name-yields-object-not-found-error
# --> habe deswegen in rawframe in SMMNR dash durch underscore ersetzt! - in 3.1
```

# 4.2 Provide further labels to SID table

1st pass scenario / 2nd pass scenario / combined 1st und 2nd pas scenario
```{r, echo = FALSE}
# provide further labels - could also be moved to SID_table section (3.2/3.3) above
# firstpass_dt: datum des ersten pass cycles vs. firstpass_scen: das FPY scenario
# flag für 1st pass scenario: es gibt ein first pass event und keine fail cycles 
SID_table$firstpass_scen <- !is.na(SID_table$firstpass_dt) & is.na(SID_table$cycle_fail)
# flag für 2nd pass scenario: es handelt sich um ein pseudo error scenario mit genau einem fail cycle
SID_table$secondpass_scen <- SID_table$pseudo & (SID_table$cycle_fail==1)  
# flag für 1st und 2nd pas scenario: beim ersten oder zweiten ICT bestanden, d.h. man kann sich den 2. Test sparen
SID_table$firstsecondpass_scen <- SID_table$firstpass_scen | SID_table$secondpass_scen

# flag für x-te pass scenario, if cascade due to NA
SID_table$xpass <- NA
for (i in 1:nrow(SID_table))  {
  if (!is.na(SID_table$cycle_fail[i])) {
    if (SID_table$pseudo[i] & (SID_table$cycle_fail[i] > 1))
      SID_table$xpass[i] <- SID_table$cycle_fail[i]
  }
}
```

# 4.3 Generate clean DF for R
```{r, echo = FALSE}
# hier noch keine Kombination mit weiteren labels 
# Eliminierung von Datensätzen mit NA Werten (nicht komplett durchgelaufene Messzyklen)
frame87_clean <- frame87_transpose %>% na.omit()

```

generate clean DF for KNIME
```{r, echo = FALSE}
frame87_KNIME <- frame87_clean
frame87_KNIME$SERGEBNIS <- as.character(frame87_KNIME$SERGEBNIS)  # KNIME wants character als label

# save clean frame fpr KNIME, etc..
write.table(frame87_KNIME,       # write resulting data into CSV file with tabstops
            file=paste0(rel_fpath, "frame87_clean", endung),
            sep=";", quote=FALSE, row.names=FALSE, dec=".") 
remove(frame87_KNIME)
```

select appropriate fields in SID_table and join with clean DF
```{r, echo = FALSE}
# combine with clean DF - innerjoin uses from SID_table only relevant info/labels - then neu sortieren
frame87_clean <- frame87_clean %>% inner_join(select(SID_table, c(SID, BEFUND_clean, pseudo, firstpass_scen, secondpass_scen, firstsecondpass_scen, xpass))) %>% select(c(NTHID, SFILENAME, DTINSERT, SID, SARTIKELNR, DTDATUM, SPLATZNR, SERGEBNIS, BEFUND_clean, pseudo, firstpass_scen, secondpass_scen, firstsecondpass_scen, xpass) , everything())

```

number of lable categories --> need for oversampling visible (topic for next sprint)
portion of errors: pseudo 10,5%, firstpass_scen 89,5%, secondpass_scen 6,4%, firstsecond_scen 95,9%, xpass 4,1%
```{r, echo = FALSE}
frame87_clean %>% group_by(SARTIKELNR, BEFUND_clean) %>% summarize(count=n())
frame87_clean %>% group_by(SARTIKELNR, pseudo) %>% summarize(count=n())
frame87_clean %>% group_by(SARTIKELNR, firstpass_scen) %>% summarize(count=n())
frame87_clean %>% group_by(SARTIKELNR, secondpass_scen) %>% summarize(count=n())
frame87_clean %>% group_by(SARTIKELNR, firstsecondpass_scen) %>% summarize(count=n())
frame87_clean %>% group_by(SARTIKELNR, xpass) %>% summarize(count=n())
```

set SERGEBNIS to 0,1  -   1 is PASS or TRUE
```{r, echo = FALSE}
frame87_clean$SERGEBNIS <- as.numeric(frame87_clean$SERGEBNIS)-1  # 1 is PASS
frame87_clean$secondpass_scen <- as.numeric(frame87_clean$secondpass_scen)  # 1 is TRUE
frame87_clean$firstsecondpass_scen <- as.numeric(frame87_clean$firstsecondpass_scen)  # 1 is TRUE
```

handling of outlieres 
5 times IQR considered as extreme outliers --> adjust estreme outier to value of extrem lower/upper whisker of 5 times IQR
```{r, echo = FALSE}
#boxplot(frame87_clean$R1211)
#boxplot.stats(frame87_clean$R1211, coef = 1.5)
frame87_cleanoutl <- frame87_clean # alternative dataframe which captures adjusted outliers

for (j in 15:ncol(frame87_cleanoutl)) {
  # statt 1.5 hier 5-facher IQR whisker
  bp <- boxplot.stats(frame87_cleanoutl[,j], coef = 5, do.conf = FALSE, do.out = FALSE)
  ext_wh <- c(bp$stats[1], bp$stats[5])
  for (i in 1:nrow(frame87_cleanoutl)) {
    if (frame87_cleanoutl[i,j] < ext_wh[1]) frame87_cleanoutl[i,j] <- ext_wh[1]
    if (frame87_cleanoutl[i,j] > ext_wh[2]) frame87_cleanoutl[i,j] <- ext_wh[2]
  }
# mutate funktioniert nicht mit Indizierung!
}
#boxplot(frame87_cleanoutl$R1211, coef = 1.5)
#boxplot.stats(frame87_cleanoutl$R1211)
```

show first 20 colums of unnormalized and clean data frame
```{r, echo = FALSE}
head(frame87_cleanoutl[,1:20])
```

normalize clean data frame
```{r, echo = FALSE}
# (KNIME hatte sogar bei log reg normalisierung angemahnt)
# optional: normalize clean data
fn_norm  = function(x) { return ( (x-min(x)) / (max(x) - min(x)) ) }  #define the rescaling function
# Kombination von nicht normierbaren Anfangsspalten mit normierbarem rest

frame87_clean = cbind.data.frame(frame87_clean[ ,(1:14)], lapply(frame87_clean[ ,-(1:14)], fn_norm)) # outlier unbehandelt
frame87_cleanoutl = cbind.data.frame(frame87_cleanoutl[ ,(1:14)], lapply(frame87_cleanoutl[ ,-(1:14)], fn_norm)) # outlier über 5-fach IQR angepasst
```
show first 20 colums of normalized and clean data frame
```{r, echo = FALSE}
head(frame87_cleanoutl[,1:20])
```
# 4.4. Feature reduction

PCA - principal component analysis (usage for feature reduction)
```{r, echo = FALSE}
data_pca <- prcomp (select(frame87_cleanoutl, -c(NTHID, SFILENAME, DTINSERT, SID, SARTIKELNR, DTDATUM, SPLATZNR, SERGEBNIS, BEFUND_clean, pseudo, firstpass_scen, secondpass_scen, firstsecondpass_scen, xpass)), center=TRUE, scale = TRUE)
print(summary (data_pca))
#print(data_pca$rotation)
plot (cumsum((data_pca$sdev)^2/sum((data_pca$sdev)^2)),
      xlab="PCA components", ylab="Cumulative Proportion of variance")
#head(data_pca$x[,1:50])

frame87_cleanpca = cbind.data.frame(frame87_cleanoutl[ ,(1:14)], data_pca$x[ ,(1:50)]) # take first 50 PCAs
frame87_cleanpca = cbind.data.frame(frame87_cleanpca [ ,(1:14)], lapply(frame87_cleanpca[ ,-(1:14)], fn_norm)) 
```

restrict clean data to first 50 most significant predictors based on VarCo investigation
```{r, echo = FALSE}
# get significant predictors for A5E02758387, starting with 100 entries, which are doubled so 50 remain
# predictors with index 1:100 are most significant, e.g. 1400:1500 are from the non significant tail
frame87_clean <- frame87_cleanoutl
sign_pred_list_87 <- unique(board_view$SMMNR[board_view$SARTIKELNR=="A5E02758387"][1:100])
frame87_clean <- cbind(frame87_clean[ ,1:14], select(frame87_clean, sign_pred_list_87))
```

split into train (70%) and test (30%)
```{r, echo = FALSE}
#frame87_clean <- frame87_cleanpca # if PCA used for feature reduction
# random splitting
NR <- nrow(frame87_clean)

set.seed(123456789)
train_index <- sample(NR, NR * 0.7)       # %-Satz der Trainingsdaten 
train <- frame87_clean[train_index, ]#-(1:1)]       # Zufallsvektor definiert die Trainingsdaten
test <- frame87_clean[-train_index, ]#-(1:1)]       # ..das Komplement die Testdaten

cat("Dimension of train data: ", dim(train))
cat("Dimension of test data: ", dim(test))
```

# 5 AI

# 5.1 AI - simple try predicting FAIL/PASS 

# 5.1.1 Logistic regression
```{r, echo = FALSE}
# linear model
#log_mod <- glm(SERGEBNIS ~. -time_pass -firstpass_scen -lastpass_dt -label_indic -pseudo -firstpass_scen -secondpass_scen -xpass, train,
#              family=binomial(link="logit"))
# aus irgendeinem Grund funktioniert der Ausschluss der Variablen über minus in formula - wie bei RF - nicht

#train <- frame87_clean # mal ohne split, nur model fit
#train$SERGEBNIS <- as.factor(train$SERGEBNIS)

#log_mod <- glm(SERGEBNIS ~. , select(train, -c(NTHID, SFILENAME, DTINSERT, SID, SARTIKELNR, DTDATUM, SPLATZNR, BEFUND_clean, pseudo, firstpass_scen, secondpass_scen, firstsecondpass_scen)), family=binomial(link="logit"))
#summary(log_mod)

#confint(log_mod)
#calculate odds
#print ("Odds ratios:")
#round(exp(coefficients(log_mod)),6) #result is odds ratio
```
glm Algorithmus konvergiert nicht bei allen predictoren, weil Anzahl predictors zu hoch.
bei Varco reduced set klappt die Konvergenz

stepwise regression feature selection
```{r, echo = FALSE}
#train <- frame87_clean   # mal ohne split, nur model fit

null <- glm(SERGEBNIS ~1 , select(train, -c(NTHID, SFILENAME, DTINSERT, SID, SARTIKELNR, DTDATUM, SPLATZNR, BEFUND_clean, pseudo, firstpass_scen, secondpass_scen, firstsecondpass_scen, xpass)), family=binomial(link="logit"))

full <- glm(SERGEBNIS ~. , select(train, -c(NTHID, SFILENAME, DTINSERT, SID, SARTIKELNR, DTDATUM, SPLATZNR, BEFUND_clean, pseudo, firstpass_scen, secondpass_scen, firstsecondpass_scen, xpass)), family=binomial(link="logit"))

log_mod <- step(null, scope = list(upper=full), data=train, direction="both")
```
```{r, echo = FALSE}
summary(log_mod)

#calculate odds
print ("Odds ratios:")
round(exp(coefficients(log_mod)),6) #result is odds ratio
```
model evaluation

prediction  
```{r, echo = FALSE}
library(e1071)
# Evaluation with test data set
yhat <- predict.glm(log_mod, select(test, -c(NTHID, SFILENAME, DTINSERT, SID, SARTIKELNR, DTDATUM, SPLATZNR, BEFUND_clean, pseudo, firstpass_scen, secondpass_scen, firstsecondpass_scen, xpass)))

#x <- runif(463, 0.0, 1.0) # test mit Zufallsfariable
#yhat_label <- as.factor(prob_to_class(x, 0.105))
# result is vector of mean values of the resprective nodes
yhat_prob <- fn_norm(yhat)        # normalize predicte values
yhat_label <- as.factor(prob_to_class(yhat_prob, 0.3))

# Confusion matrix
ytrue <- test$SERGEBNIS
caret::confusionMatrix(yhat_label, as.factor(ytrue), positive="1")

# Lift / ROC Chart for test data
par(mfrow=c(1,2))
Lift_Chart (yhat_prob, ytrue, "Lift Logistic Regression", "black")
ROC_Chart (yhat_prob, ytrue, "ROC Logistic Regression")
```
location of predictors on the board (visual clustering)

```{r, echo = FALSE, message=FALSE, warning=FALSE}
SMMNR_subtable <- SMMNR_table %>% filter(SARTIKELNR=="A5E02758387") %>% filter (SMMNR %in% names(log_mod$coefficients))
ggplot(data = SMMNR_subtable) +
  geom_point(mapping = aes(x = X_loc, y = Y_loc, colour = count_viol), size=0.8) +
#  scale_fill_gradientn(colours=c("black","yellow","red"), breaks=c(0,30,50), labels=format(c("0","30","50")) ) +
         ggtitle("Location of predictors on the board A5E02758387")
```

# 5.1.2 Random Forest. Sitches partial trees so nr. of predictors could be covered.
```{r, echo = FALSE}
# RF model https://datascienceplus.com/random-forests-in-r/
library(randomForest)
library(caret)
#train <- frame87_clean  # mal ohne split, nur model fit

RF_mod <- randomForest(as.factor(SERGEBNIS) ~., select(test, -c(NTHID, SFILENAME, DTINSERT, SID, SARTIKELNR, DTDATUM, SPLATZNR, BEFUND_clean, pseudo, firstpass_scen, secondpass_scen, firstsecondpass_scen, xpass)),importance=TRUE, ntree=500)
RF_mod
plot(RF_mod)
summary(RF_mod)

varImpPlot(RF_mod, type=1,main = "Variable Importance")
varImpPlot(RF_mod, type=2,main = "Variable Importance")

as.data.frame(RF_mod$importance) %>% mutate (Component = row.names(as.data.frame(RF_mod$importance))) %>% select(-c("0","1")) %>% arrange(desc(MeanDecreaseAccuracy))
```
error rate 1,8% differiert nicht sonderlich zwischen ganzem set der Variablen und den 50 wichtigsten (per Varco ausgewälten) 
bei Variablen aus dem 'VarCo' Schwanz steigt die error rate

Model evaluation
```{r, echo = FALSE}
library(e1071)
# Evaluation with test data set
yhat <- predict(RF_mod, select(test, -c(NTHID, SFILENAME, DTINSERT, SID, SARTIKELNR, DTDATUM, SPLATZNR, BEFUND_clean, pseudo, firstpass_scen, secondpass_scen, firstsecondpass_scen, xpass)))
# Confusion matrix
ytrue <- test$SERGEBNIS
caret::confusionMatrix(yhat_label, as.factor(ytrue), positive="1")
```

# 5.2 AI - predicting combind first and second FAIL/PASS 

# 5.2.1 Logistic Regression

stepwise regression feature selection
```{r, echo = FALSE}
#train <- frame87_clean   # mal ohne split, nur model fit

null <- glm(firstsecondpass_scen ~1 , select(train, -c(NTHID, SFILENAME, DTINSERT, SID, SARTIKELNR, DTDATUM, SPLATZNR, BEFUND_clean, SERGEBNIS, pseudo, firstpass_scen, secondpass_scen, xpass)), family=binomial(link="logit"))

full <- glm(firstsecondpass_scen ~. , select(train, -c(NTHID, SFILENAME, DTINSERT, SID, SARTIKELNR, DTDATUM, SPLATZNR, BEFUND_clean, SERGEBNIS, pseudo, firstpass_scen, secondpass_scen, xpass)), family=binomial(link="logit"))

log_mod2 <- step(null, scope = list(upper=full), data=train, direction="both")
```
```{r, echo = FALSE}
summary(log_mod2)

#calculate odds
print ("Odds ratios:")
round(exp(coefficients(log_mod2)),6) #result is odds ratio
```
model evaluation
```{r, echo = FALSE}
library(e1071)
# Evaluation with test data set
yhat <- predict.glm(log_mod2, select(test, -c(NTHID, SFILENAME, DTINSERT, SID, SARTIKELNR, DTDATUM, SPLATZNR, BEFUND_clean, SERGEBNIS ,pseudo, firstpass_scen, secondpass_scen, xpass)))

# result is vector of mean values of the resprective nodes
yhat_prob <- fn_norm(yhat)        # normalize predicte values
yhat_label <- as.factor(prob_to_class(yhat_prob, 0.20))

# Confusion matrix
ytrue <- test$firstsecondpass_scen
caret::confusionMatrix(yhat_label, as.factor(ytrue), positive="1")

# Lift / ROC Chart for test data
par(mfrow=c(1,2))
Lift_Chart (yhat_prob, ytrue, "Lift Logistic Regression", "black")
ROC_Chart (yhat_prob, ytrue, "ROC Logistic Regression")
```
location of predictors on the board (visual clustering)

```{r, echo = FALSE, message=FALSE, warning=FALSE}
SMMNR_subtable <- SMMNR_table %>% filter(SARTIKELNR=="A5E02758387") %>% filter (SMMNR %in% names(log_mod2$coefficients))
ggplot(data = SMMNR_subtable) +
  geom_point(mapping = aes(x = X_loc, y = Y_loc, colour = count_viol), size=0.8) +
#  scale_fill_gradientn(colours=c("black","yellow","red"), breaks=c(0,30,50), labels=format(c("0","30","50")) ) +
         ggtitle("Location of predictors on the board A5E02758387")
```

# 5.2.2 Random forest
```{r, echo = FALSE}

# remove first pass tests
#frame87_clean_2nd <- frame87_clean[frame87_clean$firstpass_scen==FALSE, ]
```

Random Forest. Sitches partial trees so nr. of predictors could be covered.
```{r, echo = FALSE}
library(randomForest)
library(caret)
#train <- frame87_clean   # mal ohne split, nur model fit

#train <- select(train, -c(NTHID, SFILENAME, DTINSERT, SID, SARTIKELNR, DTDATUM, SPLATZNR, SERGEBNIS, BEFUND_clean, pseudo, firstpass_scen, secondpass_scen, xpass))

# immer wieder die Probleme wenn man zuviel in formula über Minuszeichen ausschließt
#RF_mod2 <- randomForest(secondpass_scen ~. -NTHID -SFILENAME -DTINSERT -SID -SARTIKELNR -DTDATUM -SPLATZNR -SERGEBNIS -BEFUND_clean -pseudo -firstpass_scen, train, ntree=500)

RF_mod2 <- randomForest(as.factor(firstsecondpass_scen) ~. , select(train, -c(NTHID, SFILENAME, DTINSERT, SID, SARTIKELNR, DTDATUM, SPLATZNR, SERGEBNIS, BEFUND_clean, pseudo, firstpass_scen, secondpass_scen, xpass)), importance=TRUE, ntree=500)

RF_mod2
plot(RF_mod2)
summary(RF_mod2)

varImpPlot(RF_mod2, type=1,main = "Variable Importance")
varImpPlot(RF_mod2, type=2,main = "Variable Importance")

as.data.frame(RF_mod$importance) %>% mutate (Component = row.names(as.data.frame(RF_mod$importance))) %>% select(-c("0","1")) %>% arrange(desc(MeanDecreaseAccuracy))
```
5,75 % fehler bei den ersten 50 (ohne varco selection)
gleicher fehler mit allen Variablen...

Model evaluation
```{r, echo = FALSE}
library(e1071)
# Evaluation with test data set
yhat <- predict(RF_mod2, select(test, -c(NTHID, SFILENAME, DTINSERT, SID, SARTIKELNR, DTDATUM, SPLATZNR, SERGEBNIS, BEFUND_clean, pseudo, firstpass_scen, secondpass_scen, xpass)))

# Confusion matrix
ytrue <- test$firstsecondpass_scen
caret::confusionMatrix(yhat_label, as.factor(ytrue), positive="1")
```